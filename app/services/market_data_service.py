"""
ç»Ÿä¸€å¸‚åœºæ•°æ®æœåŠ¡
æä¾›è‚¡ç¥¨å¸‚åœºæ•°æ®è·å–ã€CSVæ•°æ®è¯»å†™å’Œç¼“å­˜åŠŸèƒ½
"""

import os
import time
from datetime import datetime, timedelta
from typing import Annotated, Dict, List, Optional, Any

# Add project root to Python path
import pandas as pd
from cfg import logger

from app.utils.timestamp_utils import TimestampUtils

# å¸‚åœºæ•°æ®ç¼“å­˜
_market_data_cache = {}
_cache_expiry = 30 * 60  # 30åˆ†é’Ÿ


class CSVDataService:
    """CSVæ•°æ®è¯»å†™æœåŠ¡"""
    
    # æ•°æ®å­˜å‚¨ç›®å½•
    DATA_DIR = "data/kline"
    
    @staticmethod
    def get_csv_file_path(symbol: str, time_granularity: str) -> str:
        """
        è·å–CSVæ–‡ä»¶è·¯å¾„
        
        Args:
            symbol: æ ‡çš„
            time_granularity: æ—¶é—´ç²’åº¦
            
        Returns:
            str: CSVæ–‡ä»¶è·¯å¾„
        """
        file_name = f"{symbol}_{time_granularity}_kline.csv"
        return os.path.join(CSVDataService.DATA_DIR, file_name)
    
    @staticmethod
    def read_data(symbol: str, time_granularity: str) -> pd.DataFrame:
        """
        è¯»å–CSVæ•°æ®
        
        Args:
            symbol: æ ‡çš„
            time_granularity: æ—¶é—´ç²’åº¦
            
        Returns:
            pd.DataFrame: æ•°æ®
        """
        file_path = CSVDataService.get_csv_file_path(symbol, time_granularity)
        
        if not os.path.exists(file_path):
            logger.info(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return pd.DataFrame()
        
        try:
            # å…ˆè¯»å–æ•°æ®ï¼Œä¸æŒ‡å®šæ—¥æœŸè§£æåˆ—
            df = pd.read_csv(file_path)
            
            # è§£ææ—¥æœŸåˆ—ï¼Œä¼˜å…ˆä½¿ç”¨dateåˆ—
            if 'date' in df.columns:
                df['date'] = pd.to_datetime(df['date'], errors='coerce')
            
            # åˆ é™¤ä¸éœ€è¦çš„åˆ—
            columns_to_drop = []
            if 'Symbol' in df.columns:
                columns_to_drop.append('Symbol')
            if 'symbol' in df.columns:
                columns_to_drop.append('symbol')
            
            if columns_to_drop:
                df = df.drop(columns=columns_to_drop)
                
            logger.info(f"è¯»å–æ–‡ä»¶æˆåŠŸ: {file_path}, åŒ…å« {len(df)} è¡Œæ•°æ®ï¼Œåˆ—: {list(df.columns)}")
            return df
        except Exception as e:
            logger.error(f"è¯»å–æ–‡ä»¶å¤±è´¥: {file_path}, é”™è¯¯: {e}")
            return pd.DataFrame()
    
    @staticmethod
    def write_data(df: pd.DataFrame, symbol: str, time_granularity: str) -> bool:
        """
        å†™å…¥CSVæ•°æ®
        
        Args:
            df: æ•°æ®
            symbol: æ ‡çš„
            time_granularity: æ—¶é—´ç²’åº¦
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        if df.empty:
            logger.info("æ²¡æœ‰æ•°æ®éœ€è¦å†™å…¥")
            return True
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(CSVDataService.DATA_DIR, exist_ok=True)
        
        file_path = CSVDataService.get_csv_file_path(symbol, time_granularity)
        
        try:
            # è¯»å–ç°æœ‰æ•°æ®
            existing_df = CSVDataService.read_data(symbol, time_granularity)
            
            # å¦‚æœç°æœ‰æ•°æ®ä¸ä¸ºç©ºï¼Œåˆå¹¶æ•°æ®å¹¶å»é‡
            if not existing_df.empty:
                # åˆå¹¶æ•°æ®
                combined_df = pd.concat([existing_df, df], ignore_index=True)
                # å»é‡ï¼Œæ ¹æ®dateå»é‡
                combined_df = combined_df.drop_duplicates(subset=['date'], keep='last')
                # æŒ‰dateæ’åº
                combined_df = combined_df.sort_values('date')
            else:
                combined_df = df
            
            # å†™å…¥æ•°æ®
            combined_df.to_csv(file_path, index=False, float_format='%.8f')
            logger.info(f"å†™å…¥æ–‡ä»¶æˆåŠŸ: {file_path}, åŒ…å« {len(combined_df)} è¡Œæ•°æ®")
            return True
        except Exception as e:
            logger.error(f"å†™å…¥æ–‡ä»¶å¤±è´¥: {file_path}, é”™è¯¯: {e}")
            return False
    
    @staticmethod
    def query_data(
        symbol: str,
        time_granularity: str,
        start_date: Optional[datetime] = None,
        end_date: Optional[datetime] = None
    ) -> pd.DataFrame:
        """
        æŸ¥è¯¢æ•°æ®
        
        Args:
            symbol: æ ‡çš„
            time_granularity: æ—¶é—´ç²’åº¦
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            
        Returns:
            pd.DataFrame: æŸ¥è¯¢ç»“æœ
        """
        # è¯»å–æ•°æ®
        df = CSVDataService.read_data(symbol, time_granularity)
        
        if df.empty:
            return df
        
        # åº”ç”¨è¿‡æ»¤å™¨
        filters = []
        

        
        # æ—¶é—´èŒƒå›´è¿‡æ»¤
        # ç¡®å®šä½¿ç”¨çš„æ—¥æœŸåˆ—å
        date_col = 'date'
        
        if start_date and date_col in df.columns:
            # ç¡®ä¿start_dateä¸ºnaive datetimeä»¥ä¾¿ä¸DataFrameä¸­çš„datetime64[ns]æ¯”è¾ƒ
            s_date = start_date.replace(tzinfo=None) if start_date.tzinfo else start_date
            # å¦‚æœæ˜¯æ—¥çº¿ï¼Œåªæ¯”è¾ƒæ—¥æœŸéƒ¨åˆ†
            if time_granularity == 'daily':
                s_date = s_date.replace(hour=0, minute=0, second=0, microsecond=0)
            filters.append(df[date_col] >= s_date)
        
        if end_date and date_col in df.columns:
            # ç¡®ä¿end_dateä¸ºnaive datetimeä»¥ä¾¿ä¸DataFrameä¸­çš„datetime64[ns]æ¯”è¾ƒ
            e_date = end_date.replace(tzinfo=None) if end_date.tzinfo else end_date
            # å¦‚æœæ˜¯æ—¥çº¿ï¼Œåªæ¯”è¾ƒæ—¥æœŸéƒ¨åˆ†ï¼Œä½†è¦åŒ…å«è¿™ä¸€å¤©ï¼Œæ‰€ä»¥è®¾ç½®æ—¶é—´ä¸º23:59:59
            if time_granularity == 'daily':
                e_date = e_date.replace(hour=23, minute=59, second=59, microsecond=999999)
            filters.append(df[date_col] <= e_date)
        
        # åº”ç”¨æ‰€æœ‰è¿‡æ»¤å™¨
        if filters:
            df = df[pd.concat(filters, axis=1).all(axis=1)]
        
        # æŒ‰æ—¥æœŸåˆ—æ’åº
        if date_col in df.columns:
            df = df.sort_values(date_col)
        
        return df
    
    @staticmethod
    def get_paginated_data(
        df: pd.DataFrame,
        page: int = 1,
        page_size: int = 100
    ) -> Dict[str, Any]:
        """
        è·å–åˆ†é¡µæ•°æ®
        
        Args:
            df: æ•°æ®
            page: é¡µç 
            page_size: æ¯é¡µå¤§å°
            
        Returns:
            Dict[str, Any]: åˆ†é¡µæ•°æ®
        """
        total = len(df)
        total_pages = (total + page_size - 1) // page_size
        
        # æ£€æŸ¥é¡µç æ˜¯å¦è¶…å‡ºèŒƒå›´
        if page > total_pages and total > 0:
            page = min(page, total_pages)
        
        # è®¡ç®—åç§»é‡
        offset = (page - 1) * page_size
        
        # è·å–åˆ†é¡µæ•°æ®
        paginated_df = df.iloc[offset:offset + page_size]
        
        # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
        items = []
        for _, row in paginated_df.iterrows():
            # ç¡®å®šä½¿ç”¨çš„æ—¥æœŸåˆ—å
            date_col = 'date'
            
            item = {
                "date": TimestampUtils.to_utc_iso(row[date_col]) if pd.notna(row[date_col]) else None,

                "open": str(row['open']) if pd.notna(row['open']) else None,
                "high": str(row['high']) if pd.notna(row['high']) else None,
                "low": str(row['low']) if pd.notna(row['low']) else None,
                "close": str(row['close']) if pd.notna(row['close']) else None,
                # å…¶ä»–å­—æ®µ...
            }
            
            for col in row.index:
                if col not in item:
                    value = row[col]
                    if pd.notna(value):
                        if isinstance(value, (int, float)):
                            item[col] = str(value)
                        else:
                            item[col] = value
                    else:
                        item[col] = None
            
            items.append(item)
        
        return {
            "items": items,
            "page": page,
            "page_size": page_size,
            "total": total,
            "total_pages": total_pages
        }
    
    @staticmethod
    def delete_data(symbol: str, time_granularity: str) -> bool:
        """
        åˆ é™¤æ•°æ®
        
        Args:
            symbol: æ ‡çš„
            time_granularity: æ—¶é—´ç²’åº¦
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        file_path = CSVDataService.get_csv_file_path(symbol, time_granularity)
        
        if not os.path.exists(file_path):
            logger.info(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return True
        
        try:
            os.remove(file_path)
            logger.info(f"åˆ é™¤æ–‡ä»¶æˆåŠŸ: {file_path}")
            return True
        except Exception as e:
            logger.error(f"åˆ é™¤æ–‡ä»¶å¤±è´¥: {file_path}, é”™è¯¯: {e}")
            return False
    
    @staticmethod
    def get_symbols(time_granularity: str) -> List[str]:
        """
        è·å–æ‰€æœ‰æ ‡çš„
        
        Args:
            time_granularity: æ—¶é—´ç²’åº¦
            
        Returns:
            List[str]: æ ‡çš„åˆ—è¡¨
        """
        symbols = set()
        
        # éå†æ•°æ®ç›®å½•
        for file_name in os.listdir(CSVDataService.DATA_DIR):
            if file_name.endswith(f"_{time_granularity}_kline.csv"):
                # æå–æ ‡çš„
                symbol = file_name.replace(f"_{time_granularity}_kline.csv", "")
                symbols.add(symbol)
        
        return list(symbols)

    @staticmethod
    def get_date_range(df: pd.DataFrame) -> Dict[str, Any]:
        """
        è·å–DataFrameçš„æ—¥æœŸèŒƒå›´
        
        Args:
            df: æ•°æ®DataFrame
            
        Returns:
            Dict: åŒ…å«countã€start_dateã€end_dateçš„å­—å…¸
        """
        if df.empty or 'date' not in df.columns:
            return {'count': 0, 'start_date': None, 'end_date': None}
        
        dates = pd.to_datetime(df['date'], errors='coerce').dropna()
        if dates.empty:
            return {'count': 0, 'start_date': None, 'end_date': None}
        
        return {
            'count': len(dates),
            'start_date': dates.min().strftime('%Y-%m-%d'),
            'end_date': dates.max().strftime('%Y-%m-%d')
        }


# @tool
def get_stock_market_data_unified(
        market_type: Annotated[str, "å¸‚åœºç±»å‹ï¼ˆAè‚¡ã€æ¸¯è‚¡ã€ç¾è‚¡ã€åŠ å¯†è´§å¸ï¼‰"],
        ticker: Annotated[str, "è‚¡ç¥¨æˆ–åŠ å¯†è´§å¸ä»£ç "],
        start_date: Annotated[str, "å¼€å§‹æ—¥æœŸï¼Œæ ¼å¼ï¼šYYYY-MM-DD"],
        end_date: Annotated[str, "ç»“æŸæ—¥æœŸï¼Œæ ¼å¼ï¼šYYYY-MM-DD"],
        time_granularity: Annotated[str, "æ—¶é—´ç²’åº¦ï¼šdaily/hourly/minute"],
) -> str:
    """
    ç»Ÿä¸€çš„è‚¡ç¥¨å¸‚åœºæ•°æ®å·¥å…·

    Args:
        market_type: å¸‚åœºç±»å‹ï¼ˆAè‚¡ã€æ¸¯è‚¡ã€ç¾è‚¡ã€åŠ å¯†è´§å¸ï¼‰
        ticker: è‚¡ç¥¨æˆ–åŠ å¯†è´§å¸ä»£ç ï¼ˆå¦‚ï¼š000001ã€0700.HKã€AAPLã€BTCï¼‰
        start_date: å¼€å§‹æ—¥æœŸï¼ˆæ ¼å¼ï¼šYYYY-MM-DDï¼‰
        end_date: ç»“æŸæ—¥æœŸï¼ˆæ ¼å¼ï¼šYYYY-MM-DDï¼‰
        time_granularity: æ—¶é—´ç²’åº¦ï¼šdaily/hourly/minute

    Returns:
        str: å¸‚åœºæ•°æ®å’ŒæŠ€æœ¯åˆ†ææŠ¥å‘Š
    """
    try:
        return get_market_data_txt(market_type, ticker, start_date, end_date, time_granularity)
    except Exception as e:
        error_msg = f"ç»Ÿä¸€å¸‚åœºæ•°æ®å·¥å…·æ‰§è¡Œå¤±è´¥: {str(e)}"
        logger.error(f"âŒ [ç»Ÿä¸€å¸‚åœºå·¥å…·] {error_msg}")
        return error_msg


def get_market_data_txt(market_type: str, ticker: str, start_date: str, end_date: str, time_granularity: str = "daily") -> str:
    """
    ç»Ÿä¸€çš„å¸‚åœºæ•°æ®å·¥å…·
    :param market_type: å¸‚åœºç±»å‹ï¼ˆå¦‚ï¼šAè‚¡ã€æ¸¯è‚¡ã€ç¾è‚¡ã€åŠ å¯†è´§å¸ï¼‰
    :param ticker: ä»£ç ï¼ˆå¦‚ï¼š000001ã€0700.HKã€AAPLã€BTCï¼‰
    :param start_date: å¼€å§‹æ—¥æœŸï¼ˆæ ¼å¼ï¼šYYYY-MM-DDï¼‰
    :param end_date: ç»“æŸæ—¥æœŸï¼ˆæ ¼å¼ï¼šYYYY-MM-DDï¼‰
    :param time_granularity: æ—¶é—´ç²’åº¦ï¼šdaily/hourly/minute
    :return: str: å¸‚åœºæ•°æ®æŠ¥å‘Š
    """
    
    logger.info(f"ğŸ“ˆ [ç»Ÿä¸€å¸‚åœºå·¥å…·] å¤„ç†{market_type} {ticker}å¸‚åœºæ•°æ®...")
    
    # 1. æ£€æŸ¥ç¼“å­˜ï¼Œæ·»åŠ æ—¶é—´ç²’åº¦åˆ°ç¼“å­˜é”®
    cache_key = f"{market_type}_{ticker}_{start_date}_{end_date}_{time_granularity}"
    if cache_key in _market_data_cache:
        cached_data, timestamp = _market_data_cache[cache_key]
        if time.time() - timestamp < _cache_expiry:
            logger.info(f"ğŸ“‹ [ç»Ÿä¸€å¸‚åœºå·¥å…·] ä»ç¼“å­˜è·å–æ•°æ®: {cache_key}")
            return cached_data
    
    try:
        from app.utils.timestamp_utils import TimestampUtils
        start_date, end_date = TimestampUtils.std_date_range(start_date, end_date)
        result_data = []
        ds = "æ ¹æ®è‚¡ç¥¨ç±»å‹è‡ªåŠ¨é€‰æ‹©æœ€é€‚åˆçš„æ•°æ®æº"
        
        # 2. ä»CSVæ–‡ä»¶è·å–æ•°æ®
        logger.info(f"ğŸ“ [ç»Ÿä¸€å¸‚åœºå·¥å…·] ä»CSVæ–‡ä»¶è·å–æ•°æ®: {market_type} {ticker} {start_date}~{end_date} ç²’åº¦: {time_granularity}")
        
        # è½¬æ¢æ—¥æœŸæ ¼å¼
        start_date_dt = datetime.strptime(start_date, "%Y-%m-%d")
        end_date_dt = datetime.strptime(end_date, "%Y-%m-%d")
        
        # ä½¿ç”¨CSVæ•°æ®æœåŠ¡è·å–æ•°æ®
        df = CSVDataService.query_data(
            symbol=ticker,
            time_granularity=time_granularity,
            start_date=start_date_dt,
            end_date=end_date_dt
        )
        
        if not df.empty:
            logger.info(f"âœ… [ç»Ÿä¸€å¸‚åœºå·¥å…·] ä»CSVæ–‡ä»¶è·å–åˆ° {len(df)} æ¡æ•°æ®")
            
            # æ„å»ºç®€å•æ•°æ®è¡¨æ ¼
            data_str = f"## {market_type} {ticker} å¸‚åœºæ•°æ®åˆ†æ\n\n"
            data_str += f"**åˆ†ææœŸé—´**: {start_date} è‡³ {end_date}\n\n"
            
            # è·å–æ‰€æœ‰å¿…è¦çš„å­—æ®µå¹¶æ˜¾ç¤ºæ•°æ®
            field_names = ['date', 'open', 'high', 'low', 'close', 'volume',
                          'change', 'pct_chg', 'amplitude',
                          'close_5_sma', 'close_20_sma', 'close_50_sma', 'close_60_sma', 'close_200_sma',
                          'close_12_ema', 'close_26_ema', 'macd', 'macds', 'macdh',
                          'rsi_6', 'rsi_12', 'rsi_24', 'kdjk', 'kdjd', 'kdjj',
                          'boll', 'boll_ub', 'boll_lb', 'volume_5_sma', 'volume_10_sma']
            
            # æ£€æŸ¥å“ªäº›å­—æ®µå­˜åœ¨
            available_fields = []
            for field_name in field_names:
                if field_name in df.columns and not df[field_name].isnull().all():
                    available_fields.append(field_name)
            
            # åˆ›å»ºè¡¨æ ¼å¤´
            data_str += "| " + " | ".join([f.replace('_', ' ').title() for f in available_fields]) + " |\n"
            data_str += "|" + "------|" * len(available_fields) + "\n"
            
            # æ·»åŠ æ•°æ®è¡Œï¼ˆæ˜¾ç¤ºå‰10æ¡ï¼‰
            for _, row in df.head(10).iterrows():
                row_data = []
                for field_name in available_fields:
                    value = row[field_name]
                    if field_name == 'date':
                        row_data.append(row[field_name].strftime('%Y-%m-%d %H:%M:%S'))
                    elif isinstance(value, (int, float)):
                        row_data.append(f"{value:.4f}")
                    else:
                        row_data.append(str(value) if pd.notna(value) else '-')
                data_str += "| " + " | ".join(row_data) + " |\n"
            
            result_data.append(data_str)
            ds = "CSV File"
            
            # å­˜å…¥ç¼“å­˜
            final_result = f"**{ticker} å¸‚åœºæ•°æ®åˆ†æ**\n\n"
            final_result += f"**åˆ†ææœŸé—´**: {start_date} è‡³ {end_date}\n\n"
            final_result += f"{chr(10).join(result_data)}\n\n"
            final_result += f"*æ•°æ®æ¥æº: {ds}*\n"
            
            _market_data_cache[cache_key] = (final_result, time.time())
            return final_result
        
        # 5. ç»„ç»‡æœ€ç»ˆç»“æœ
        final_result = f"**{ticker} å¸‚åœºæ•°æ®åˆ†æ**\n\n"
        final_result += f"**åˆ†ææœŸé—´**: {start_date} è‡³ {end_date}\n\n"
        final_result += f"{chr(10).join(result_data)}\n\n"
        if "æ•°æ®æ¥æº" not in final_result:
            final_result += f"*æ•°æ®æ¥æº: {ds}*\n"
        
        # 6. å­˜å…¥ç¼“å­˜
        _market_data_cache[cache_key] = (final_result, time.time())
        return final_result
        
    except Exception as e:
        error_msg = f"ç»Ÿä¸€å¸‚åœºæ•°æ®å·¥å…·æ‰§è¡Œå¤±è´¥: {str(e)}"
        logger.error(f"âŒ [ç»Ÿä¸€å¸‚åœºå·¥å…·] {error_msg}")
        return error_msg