"""
ç»Ÿä¸€å¸‚åœºæ•°æ®å¯¼å…¥æœåŠ¡
æ”¯æŒæ—¥çº¿ã€å°æ—¶çº¿ã€åˆ†é’Ÿçº¿æ•°æ®å¯¼å…¥
"""
import io
from typing import Dict, Any, List, Tuple

import pandas as pd

from app.utils.calc_utils import calc_indicators
from cfg import logger


class MarketDataImportService:
    """ç»Ÿä¸€å¸‚åœºæ•°æ®å¯¼å…¥æœåŠ¡"""
    
    # å¿…éœ€çš„åˆ—
    REQUIRED_COLUMNS = ['date', 'open', 'close', 'high', 'low', 'volume']
    
    # åˆ—åæ˜ å°„é…ç½® - ç³»ç»Ÿåˆ—å: [å¯èƒ½çš„ç”¨æˆ·åˆ—ååˆ—è¡¨]
    # æ³¨æ„ï¼švolumeå¯¹åº”Volume USDTåˆ—
    COLUMN_MAPPING = {
        'date': ['date'],
        'open': ['open'],
        'high': ['high'],
        'low': ['low'],
        'close': ['close'],
        'volume': ['volume usdt']  # ç³»ç»Ÿvolumeå¯¹åº”ç”¨æˆ·çš„Volume USDT
        # ç§»é™¤äº†tradecountæ˜ å°„ï¼Œä¸å†å¯¼å…¥è¯¥åˆ—
    }
    
    @staticmethod
    def _normalize_column_name(col_name: str) -> str:
        """
        è§„èŒƒåŒ–åˆ—åï¼Œç”¨äºåŒ¹é…æ˜ å°„
        
        Args:
            col_name: åŸå§‹åˆ—å
            
        Returns:
            str: è§„èŒƒåŒ–åçš„åˆ—å
        """
        return col_name.lower().strip()
    
    @staticmethod
    def _map_columns(df: pd.DataFrame) -> pd.DataFrame:
        """
        å°†ç”¨æˆ·CSVåˆ—æ˜ å°„åˆ°ç³»ç»Ÿè¦æ±‚çš„åˆ—ï¼Œåªä¿ç•™å¿…è¦çš„åˆ—
        
        Args:
            df: åŸå§‹æ•°æ®
            
        Returns:
            pd.DataFrame: æ˜ å°„åçš„æ•°æ®
        """
        # å®šä¹‰å¿…è¦çš„åˆ—
        required_columns = ['date', 'open', 'close', 'high', 'low', 'volume']
        
        # åˆ›å»ºä¸€ä¸ªç©ºçš„DataFrameï¼ŒåªåŒ…å«å¿…è¦çš„åˆ—
        mapped_df = pd.DataFrame()
        
        # ä¿å­˜åŸå§‹åˆ—ååˆ°è§„èŒƒåŒ–åˆ—åçš„æ˜ å°„
        normalized_columns = {MarketDataImportService._normalize_column_name(col): col for col in df.columns}
        
        # åå‘æ˜ å°„ï¼šè§„èŒƒåŒ–åçš„åˆ—å -> ç³»ç»Ÿåˆ—å
        reverse_mapping = {}
        for sys_col, user_cols in MarketDataImportService.COLUMN_MAPPING.items():
            for user_col in user_cols:
                reverse_mapping[user_col.lower()] = sys_col
        
        # æ˜ å°„åˆ—åï¼Œä¼˜å…ˆå¤„ç†Volume USDTå’Œæ—¥æœŸåˆ—
        for normalized_col, original_col in normalized_columns.items():
            # ç²¾ç¡®å¤„ç†Volume USDT
            if normalized_col == 'volume usdt':
                # Volume USDT -> volume
                mapped_df['volume'] = df[original_col]
            elif normalized_col in reverse_mapping:
                # å¤„ç†å…¶ä»–åˆ—æ˜ å°„ï¼ŒåŒ…æ‹¬æ—¥æœŸåˆ—
                sys_col = reverse_mapping[normalized_col]
                # åªæ˜ å°„å¿…è¦çš„åˆ—
                if sys_col in required_columns:
                    mapped_df[sys_col] = df[original_col]
        
        # ä»åŸå§‹æ•°æ®ä¸­ç›´æ¥å¤åˆ¶å¿…è¦çš„åˆ—
        for col in required_columns:
            if col in df.columns and col not in mapped_df.columns:
                mapped_df[col] = df[col]
        
        return mapped_df
    
    @staticmethod
    def validate_csv_columns(columns: List[str]) -> Tuple[bool, List[str]]:
        """
        éªŒè¯CSVæ–‡ä»¶æ˜¯å¦åŒ…å«å¿…éœ€çš„åˆ—
        æ”¯æŒçµæ´»çš„åˆ—ååŒ¹é…ï¼Œåªè¦èƒ½å¤Ÿæ˜ å°„åˆ°ç³»ç»Ÿè¦æ±‚çš„åˆ—åå³å¯
        
        Args:
            columns: CSVæ–‡ä»¶çš„åˆ—ååˆ—è¡¨ï¼ˆå¯ä»¥æ˜¯åŸå§‹åˆ—åæˆ–æ˜ å°„åçš„åˆ—åï¼‰
            
        Returns:
            Tuple[bool, List[str]]: éªŒè¯ç»“æœå’Œé”™è¯¯ä¿¡æ¯åˆ—è¡¨
        """
        errors = []
        
        # è§„èŒƒåŒ–æ‰€æœ‰è¾“å…¥åˆ—å
        normalized_columns = {MarketDataImportService._normalize_column_name(col): col for col in columns}
        
        # æ£€æŸ¥å¿…éœ€åˆ—æ˜¯å¦å­˜åœ¨ï¼Œsymbolå­—æ®µç”±ç³»ç»Ÿè‡ªåŠ¨å¤„ç†ï¼Œä¸éœ€è¦éªŒè¯
        for req_col in MarketDataImportService.REQUIRED_COLUMNS:
            # æ£€æŸ¥æ˜ å°„åçš„åˆ—æ˜¯å¦ç›´æ¥åŒ…å«å¿…éœ€åˆ—
            if req_col.lower() in normalized_columns:
                continue
            
            # å¦‚æœæ²¡æœ‰ç›´æ¥åŒ…å«ï¼Œæ£€æŸ¥æ˜¯å¦å¯ä»¥é€šè¿‡æ˜ å°„æ‰¾åˆ°
            has_mapping = False
            for sys_col, user_cols in MarketDataImportService.COLUMN_MAPPING.items():
                if sys_col == req_col:
                    for user_col in user_cols:
                        if user_col.lower() in normalized_columns:
                            has_mapping = True
                            break
                if has_mapping:
                    break
            
            if not has_mapping:
                errors.append(f"ç¼ºå°‘å¿…éœ€åˆ—: {req_col}")
        
        return len(errors) == 0, errors
    
    @staticmethod
    def process_data(df: pd.DataFrame, time_granularity: str) -> pd.DataFrame:
        """
        å¤„ç†æ•°æ®ï¼Œè®¡ç®—æŒ‡æ ‡
        
        Args:
            df: åŸå§‹æ•°æ®DataFrame
            time_granularity: æ—¶é—´ç²’åº¦
            
        Returns:
            pd.DataFrame: å¤„ç†åçš„æ•°æ®
        """
        logger.info(f"ğŸ“‹ å¼€å§‹å¤„ç†æ•°æ®ï¼ŒåŸå§‹æ•°æ®åŒ…å« {len(df)} è¡Œï¼Œåˆ—: {list(df.columns)}")
        
        # 1. æ˜ å°„åˆ—å
        df = MarketDataImportService._map_columns(df)
        logger.info(f"ğŸ“‹ åˆ—åæ˜ å°„åï¼Œæ•°æ®åŒ…å« {len(df)} è¡Œï¼Œåˆ—: {list(df.columns)}")
        
        # 2. ç¡®ä¿æ—¥æœŸåˆ—æ ¼å¼æ­£ç¡®
        # åªä½¿ç”¨dateåˆ—
        if 'date' in df.columns:
            df['date'] = pd.to_datetime(df['date'], errors='coerce')
        else:
            raise ValueError("ç¼ºå°‘ 'date' åˆ—")
        
        # è¿‡æ»¤æ‰æ—¥æœŸè½¬æ¢å¤±è´¥çš„è¡Œ
        df = df.dropna(subset=['date'])
        logger.info(f"ğŸ“‹ æ—¥æœŸè½¬æ¢åï¼Œæ•°æ®åŒ…å« {len(df)} è¡Œ")
        
        # å»é‡ï¼Œç¡®ä¿æ¯ä¸ªdateåªå‡ºç°ä¸€æ¬¡
        df = df.drop_duplicates(subset=['date'])
        logger.info(f"ğŸ“‹ å»é‡åï¼Œæ•°æ®åŒ…å« {len(df)} è¡Œ")
        
        # æ‰§è¡ŒæŒ‡æ ‡è®¡ç®—
        processed_df = calc_indicators(df)
        
        # è®¡ç®—å®Œæˆååˆ é™¤trade_dateåˆ—ï¼Œåªä¿ç•™dateåˆ—
        logger.info(f"ğŸ“‹ æŒ‡æ ‡è®¡ç®—åï¼Œæ•°æ®åŒ…å« {len(processed_df)} è¡Œ")
        
        return processed_df
    
    @staticmethod
    def batch_upsert(
        df: pd.DataFrame,
        symbol: str,
        time_granularity: str
    ) -> Dict[str, int]:
        """
        æ‰¹é‡æ’å…¥æˆ–æ›´æ–°æ•°æ®
        
        Args:
            df: å¤„ç†åçš„æ•°æ®
            symbol: æ ‡çš„
            time_granularity: æ—¶é—´ç²’åº¦
            
        Returns:
            Dict[str, int]: æ’å…¥å’Œæ›´æ–°çš„è®°å½•æ•°
        """
        from app.services.market_data_service import CSVDataService
        
        if df.empty:
            return {'inserted': 0, 'updated': 0}
        
        # ç¡®ä¿dateåˆ—å­˜åœ¨
        if 'date' not in df.columns:
            raise ValueError("dateåˆ—ä¸å­˜åœ¨")
        
        # è¯»å–ç°æœ‰æ•°æ®
        existing_df = CSVDataService.read_data(symbol, time_granularity)
        
        # è®¡ç®—æ’å…¥å’Œæ›´æ–°çš„è®°å½•æ•°
        if existing_df.empty:
            inserted = len(df)
            updated = 0
        else:
            # åˆå¹¶æ•°æ®å¹¶å»é‡
            combined_df = pd.concat([existing_df, df], ignore_index=True)
            combined_df = combined_df.drop_duplicates(subset=['date'], keep='last')
            
            # è®¡ç®—æ’å…¥å’Œæ›´æ–°çš„è®°å½•æ•°
            existing_dates = set(existing_df['date'])
            new_dates = set(df['date'])
            
            inserted = len(new_dates - existing_dates)
            updated = len(new_dates & existing_dates)
        
        # å†™å…¥æ•°æ®åˆ°CSVæ–‡ä»¶
        success = CSVDataService.write_data(df, symbol, time_granularity)
        
        if not success:
            return {'inserted': 0, 'updated': 0}
        
        return {
            'inserted': inserted,
            'updated': updated
        }
    
    @staticmethod
    def import_data(
        file_content: bytes,
        time_granularity: str,
        symbol: str,
        max_file_size: int = 200 * 1024 * 1024  # 20MB
    ) -> Dict[str, Any]:
        """
        å¯¼å…¥å¸‚åœºæ•°æ®
        
        Args:
            file_content: CSVæ–‡ä»¶å†…å®¹
            time_granularity: æ—¶é—´ç²’åº¦ï¼ˆdaily/hourly/minuteï¼‰
            symbol: æ ‡çš„
            max_file_size: æœ€å¤§æ–‡ä»¶å¤§å°
            
        Returns:
            Dict[str, Any]: å¯¼å…¥ç»“æœ
        """
        try:
            # æ£€æŸ¥æ–‡ä»¶å¤§å°
            if len(file_content) > max_file_size:
                return {
                    'success': False,
                    'code': 413,
                    'message': "æ–‡ä»¶å¤§å°è¶…è¿‡é™åˆ¶ï¼Œæœ€å¤§å…è®¸200MB",
                    'data': None
                }
            
            # è¯»å–CSVæ•°æ®
            df = pd.read_csv(io.StringIO(file_content.decode("utf-8")))
            logger.info(f"ğŸ“‹ è¯»å–CSVæ•°æ®æˆåŠŸï¼ŒåŒ…å« {len(df)} è¡Œï¼ŒåŸå§‹åˆ—: {list(df.columns)}")
            
            # ä¿å­˜åŸå§‹åˆ—åï¼Œç”¨äºæœ€ç»ˆè¾“å‡º
            original_columns = list(df.columns)
            
            # 1. æ˜ å°„åˆ—ååˆ°ç³»ç»Ÿè¦æ±‚çš„åˆ—
            mapped_df = MarketDataImportService._map_columns(df)
            logger.info(f"ğŸ“‹ åˆ—åæ˜ å°„åï¼Œæ•°æ®åŒ…å« {len(mapped_df)} è¡Œï¼Œæ˜ å°„åˆ—: {list(mapped_df.columns)}")
            
            # 2. éªŒè¯å¿…éœ€åˆ—
            is_valid, errors = MarketDataImportService.validate_csv_columns(list(mapped_df.columns))
            if not is_valid:
                return {
                    'success': False,
                    'code': 400,
                    'message': f"CSVæ–‡ä»¶éªŒè¯å¤±è´¥: {', '.join(errors)}",
                    'data': None
                }
            

            
            # 3. å¤„ç†æ•°æ®å’Œè®¡ç®—æŒ‡æ ‡
            processed_df = MarketDataImportService.process_data(mapped_df, time_granularity)
            logger.info(f"ğŸ“‹ æ•°æ®å¤„ç†å®Œæˆï¼ŒåŒ…å« {len(processed_df)} è¡Œï¼Œå¤„ç†ååˆ—: {list(processed_df.columns)}")
            
            # 4. ç¡®ä¿æœ€ç»ˆè¾“å‡ºåŒ…å«ç”¨æˆ·è¦æ±‚çš„æ ‡å‡†åˆ—
            # ç”¨æˆ·è¦æ±‚çš„æ ‡å‡†åˆ—: date, open, high, low, close, volume
            standard_columns = [
                'date', 'open', 'high', 'low', 'close', 
                'volume'
            ]
            
            # æ·»åŠ ç¼ºå¤±çš„æ ‡å‡†åˆ—
            for col in standard_columns:
                if col not in processed_df.columns:
                    # å°è¯•ä»åŸå§‹æ•°æ®ä¸­è·å–
                    found = False
                    for original_col in original_columns:
                        if MarketDataImportService._normalize_column_name(original_col) == col:
                            processed_df[col] = df[original_col]
                            found = True
                            break
                    # å¦‚æœåŸå§‹æ•°æ®ä¸­æ²¡æœ‰ï¼Œåˆ™åˆ›å»ºç©ºåˆ—
                    if not found:
                        processed_df[col] = None
            
            # 5. é‡å‘½ååˆ—åï¼Œç¡®ä¿ä¸ç³»ç»Ÿè¦æ±‚çš„å®Œå…¨ä¸€è‡´
            column_rename_map = {
                'volume usdt': 'volume'  # Volume USDTåˆ—æ˜ å°„ä¸ºvolume
            }
            
            for old_col, new_col in column_rename_map.items():
                if old_col in processed_df.columns:
                    processed_df[new_col] = processed_df[old_col]
            
            # 6. åˆ é™¤ä¸éœ€è¦çš„åˆ—
            columns_to_drop = []
            if 'unix' in processed_df.columns:
                columns_to_drop.append('unix')
            if 'volume_btc' in processed_df.columns:
                columns_to_drop.append('volume_btc')
            if 'symbol' in processed_df.columns:
                columns_to_drop.append('symbol')
            if 'tradecount' in processed_df.columns:
                columns_to_drop.append('tradecount')
            
            if columns_to_drop:
                processed_df = processed_df.drop(columns=columns_to_drop)
            
            # 8. ç¡®ä¿æ—¥æœŸæ ¼å¼æ­£ç¡®
            if 'date' in processed_df.columns:
                processed_df['date'] = pd.to_datetime(processed_df['date'])
            
            # 9. æ‰¹é‡æ’å…¥æˆ–æ›´æ–°æ•°æ®
            result = MarketDataImportService.batch_upsert(processed_df, symbol, time_granularity)
            logger.info(f"ğŸ“‹ æ‰¹é‡æ“ä½œå®Œæˆï¼Œæ’å…¥ {result['inserted']} è¡Œï¼Œæ›´æ–° {result['updated']} è¡Œ")
            
            logger.info(f"âœ… æ•°æ®å¯¼å…¥æˆåŠŸï¼Œæ€»å…±å¤„ç† {result['inserted'] + result['updated']} è¡Œ")
            
            return {
                'success': True,
                'code': 200,
                'message': "æ•°æ®å¯¼å…¥æˆåŠŸ",
                'data': {
                    'inserted_count': result['inserted'],
                    'updated_count': result['updated']
                }
            }
        except Exception as e:
            logger.error(f"âŒ å¯¼å…¥å¸‚åœºæ•°æ®å¤±è´¥: {e}", exc_info=True)
            return {
                'success': False,
                'code': 500,
                'message': f"æ•°æ®å¯¼å…¥å¤±è´¥: {str(e)}",
                'data': None
            }
    
    @staticmethod
    def read_csv(file_content: bytes, encoding: str = 'utf-8') -> Tuple[List[str], pd.DataFrame]:
        """
        è¯»å–CSVæ–‡ä»¶å†…å®¹ï¼Œè¿”å›åˆ—åå’Œæ•°æ®
        
        Args:
            file_content: CSVæ–‡ä»¶å†…å®¹
            encoding: æ–‡ä»¶ç¼–ç 
            
        Returns:
            Tuple[List[str], pd.DataFrame]: åˆ—ååˆ—è¡¨å’Œæ•°æ®DataFrame
        """
        try:
            # è¯»å–CSVæ–‡ä»¶
            df = pd.read_csv(io.BytesIO(file_content), encoding=encoding)
            
            # è·å–åˆ—å
            columns = df.columns.tolist()
            
            logger.info(f"ğŸ“‹ æˆåŠŸè¯»å–CSVæ–‡ä»¶ï¼ŒåŒ…å« {len(columns)} åˆ—ï¼Œ {len(df)} è¡Œæ•°æ®")
            
            return columns, df
        except Exception as e:
            logger.error(f"âŒ è¯»å–CSVæ–‡ä»¶å¤±è´¥: {str(e)}")
            raise
    
    @staticmethod
    def validate_mapping(csv_columns: List[str], mapping: Dict[str, str]) -> Tuple[bool, List[str]]:
        """
        éªŒè¯åˆ—æ˜ å°„å…³ç³»çš„åˆæ³•æ€§
        
        Args:
            csv_columns: CSVåˆ—ååˆ—è¡¨
            mapping: æ˜ å°„å…³ç³»ï¼Œé”®ä¸ºCSVåˆ—åï¼Œå€¼ä¸ºç›®æ ‡åˆ—å
            
        Returns:
            Tuple[bool, List[str]]: éªŒè¯ç»“æœå’Œé”™è¯¯ä¿¡æ¯åˆ—è¡¨
        """
        errors = []
        
        # è¿‡æ»¤æ‰ç©ºå€¼æ˜ å°„ï¼ˆç”¨æˆ·é€‰æ‹©äº†"ä¸æ˜ å°„"çš„æƒ…å†µï¼‰
        filtered_mapping = {k: v for k, v in mapping.items() if v}
        
        # æ£€æŸ¥æ˜ å°„ä¸­çš„CSVåˆ—æ˜¯å¦å­˜åœ¨
        for csv_col in filtered_mapping.keys():
            if csv_col not in csv_columns:
                errors.append(f"CSVåˆ— '{csv_col}' ä¸å­˜åœ¨")
        
        # æ£€æŸ¥å¿…éœ€çš„å­—æ®µæ˜¯å¦éƒ½æœ‰æ˜ å°„
        required_fields = ['open', 'close', 'high', 'low', 'volume', 'date']
        mapped_fields = set(filtered_mapping.values())
        
        for field in required_fields:
            if field not in mapped_fields:
                errors.append(f"å¿…éœ€å­—æ®µ '{field}' å¿…é¡»æ˜ å°„")
        
        is_valid = len(errors) == 0
        
        if is_valid:
            logger.info("âœ… åˆ—æ˜ å°„éªŒè¯é€šè¿‡")
        else:
            logger.warning(f"âš ï¸  åˆ—æ˜ å°„éªŒè¯å¤±è´¥: {', '.join(errors)}")
        
        return is_valid, errors
    
    @staticmethod
    def generate_preview(df: pd.DataFrame, limit: int = 10) -> List[Dict[str, Any]]:
        """
        ç”Ÿæˆæ•°æ®é¢„è§ˆ
        
        Args:
            df: æ•°æ®DataFrame
            limit: é¢„è§ˆè¡Œæ•°
            
        Returns:
            List[Dict[str, Any]]: é¢„è§ˆæ•°æ®åˆ—è¡¨
        """
        try:
            # è·å–å‰Nè¡Œæ•°æ®
            preview_df = df.head(limit)
            
            # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
            preview_data = preview_df.to_dict('records')
            
            logger.info(f"ğŸ“‹ ç”Ÿæˆæ•°æ®é¢„è§ˆï¼ŒåŒ…å« {len(preview_data)} è¡Œæ•°æ®")
            
            return preview_data
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆæ•°æ®é¢„è§ˆå¤±è´¥: {str(e)}")
            raise
    
    @staticmethod
    def suggest_mapping(csv_columns: List[str]) -> Dict[str, str]:
        """
        è‡ªåŠ¨ç”Ÿæˆåˆ—æ˜ å°„å»ºè®®
        
        Args:
            csv_columns: CSVåˆ—ååˆ—è¡¨
            
        Returns:
            Dict[str, str]: å»ºè®®çš„æ˜ å°„å…³ç³»
        """
        mapping = {}
        
        # å¿…éœ€çš„ç›®æ ‡å­—æ®µ
        target_fields = ['open', 'close', 'high', 'low', 'volume', 'date']
        
        # å°è¯•ç›´æ¥åŒ¹é…ç›¸åŒçš„åˆ—å
        for csv_col in csv_columns:
            csv_col_lower = csv_col.lower()
            
            # ç‰¹æ®Šå¤„ç†æ—¥æœŸåˆ—
            if csv_col_lower in ['date', 'time', 'datetime']:
                mapping[csv_col] = 'date'
            # å¤„ç†å…¶ä»–å¿…éœ€å­—æ®µ
            elif csv_col_lower in target_fields:
                mapping[csv_col] = csv_col_lower
            # å¤„ç†æˆäº¤é‡åˆ—
            elif csv_col_lower == 'volume usdt':
                mapping[csv_col] = 'volume'
        
        logger.info(f"ğŸ“‹ ç”Ÿæˆæ˜ å°„å»ºè®®ï¼Œè‡ªåŠ¨åŒ¹é… {len(mapping)} åˆ—")
        
        return mapping
